{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/iacs.png\"> CS109A Introduction to Data Science: \n",
    "\n",
    "## Homework 3  AC 209 : Regularization\n",
    "\n",
    "\n",
    "**Harvard University**<br/>\n",
    "**Fall 2019**<br/>\n",
    "**Instructors**: Pavlos Protopapas, Kevin Rader and Chris Tanner\n",
    "\n",
    "<hr style=\"height:2pt\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "blockquote { background: #AEDE94; }\n",
       "h1 { \n",
       "    padding-top: 25px;\n",
       "    padding-bottom: 25px;\n",
       "    text-align: left; \n",
       "    padding-left: 10px;\n",
       "    background-color: #DDDDDD; \n",
       "    color: black;\n",
       "}\n",
       "h2 { \n",
       "    padding-top: 10px;\n",
       "    padding-bottom: 10px;\n",
       "    text-align: left; \n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE; \n",
       "    color: black;\n",
       "}\n",
       "\n",
       "div.exercise {\n",
       "\tbackground-color: #ffcccc;\n",
       "\tborder-color: #E9967A; \t\n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "}\n",
       "\n",
       "span.sub-q {\n",
       "\tfont-weight: bold;\n",
       "}\n",
       "div.theme {\n",
       "\tbackground-color: #DDDDDD;\n",
       "\tborder-color: #E9967A; \t\n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "\tfont-size: 18pt;\n",
       "}\n",
       "div.gc { \n",
       "\tbackground-color: #AEDE94;\n",
       "\tborder-color: #E9967A; \t \n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "\tfont-size: 12pt;\n",
       "}\n",
       "p.q1 { \n",
       "    padding-top: 5px;\n",
       "    padding-bottom: 5px;\n",
       "    text-align: left; \n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE; \n",
       "    color: black;\n",
       "}\n",
       "header {\n",
       "   padding-top: 35px;\n",
       "    padding-bottom: 35px;\n",
       "    text-align: left; \n",
       "    padding-left: 10px;\n",
       "    background-color: #DDDDDD; \n",
       "    color: black;\n",
       "}\n",
       "</style>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RUN THIS CELL FOR FORMAT\n",
    "import requests\n",
    "from IPython.core.display import HTML\n",
    "styles = requests.get(\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/cs109.css\").text\n",
    "HTML(styles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, RidgeCV, LassoCV, ElasticNetCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise'> <b> Question 1 [12 pts] </b> </div>\n",
    "\n",
    "Ridge and LASSO regularizations are powerful tools that not only increase generalization, but also expand the range of problems that we can solve. We will study this statement in this question. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.1** Let $X\\in \\mathbb{R}^{n\\times p}$ be a matrix of observations, where each row corresponds an observation and each column corresponds to a predictor. Now consider the case $p > n$: explain why there is no unique solution to the OLS estimator. \n",
    "\n",
    "**1.2**  Now consider the Ridge formulation. Show that finding the ridge estimator is equivalent to solving an OLS problem after adding p dummy observations with their X value equal to $\\sqrt{\\lambda}$ at the j-th component and zero everywhere else, and their Y value set to zero. In a nutshell, show that the ridge estimator can be found by getting the least squares estimator for the augmented problem:\n",
    "\n",
    "$$X^* = \\begin{bmatrix} X \\\\ \\sqrt{\\lambda}I \\end{bmatrix}$$\n",
    "\n",
    "$$Y^* = \\begin{bmatrix} Y \\\\ \\textbf{0} \\end{bmatrix}$$\n",
    "\n",
    "**1.3** Can we now solve the $p > n$ situation? Explain why.\n",
    "\n",
    "**1.4** Take a look at the LASSO estimator expression that we derived when $X^TX=I$. What needs to happen for LASSO to nullify  $\\beta_i$?\n",
    "\n",
    "**1.5**  Can LASSO be used when $p>n$? What important consideration, related to the number of predictors that LASSO chooses, do we have to keep in mind in that case?\n",
    "\n",
    "**1.6** Ridge and LASSO still have room for improvement. List two limitations of Ridge, and two limitations of LASSO.\n",
    "\n",
    "**1.7** Review the class slides and answer the following questions: When is Ridge preferred? When is LASSO preferred? When is Elastic Net preferred?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.1 Let $X\\in \\mathbb{R}^{n\\times p}$ be a matrix of observations, where each row corresponds an observation and each column corresponds to a predictor. Now consider the case $p > n$: explain why there is no unique solution to the OLS estimator. **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no unique solution to the OLS estimator in the case where p>n because there are an infinite number of possible solutions that minimize the cost/loss function. In order to be effective, OLS requires at least the same number of predictors as observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.2  Now consider the Ridge formulation. Show that finding the ridge estimator is equivalent to solving an OLS problem after adding p dummy observations with their X value equal to $\\sqrt{\\lambda}$ at the j-th component and zero everywhere else, and their Y value set to zero. In a nutshell, show that the ridge estimator can be found by getting the least squares estimator for the augmented problem: **\n",
    "\n",
    "$$X^* = \\begin{bmatrix} X \\\\ \\sqrt{\\lambda}I \\end{bmatrix}$$\n",
    "\n",
    "$$Y^* = \\begin{bmatrix} Y \\\\ \\textbf{0} \\end{bmatrix}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The least square estimator for the augmented problem above is:\n",
    "$\\beta$ = ($X^*$$^T$$X^*$)$^-1$$X^*$$^T$$Y^*$\n",
    "\n",
    "\n",
    "Equivalently:\n",
    "$\\beta$ = ([$X$ $\\sqrt{\\lambda}I$] $\\begin{bmatrix} X \\\\ \\sqrt{\\lambda}I \\end{bmatrix}$)$^-1$[$X$ $\\sqrt{\\lambda}I$]$\\begin{bmatrix} Y \\\\ \\textbf{0} \\end{bmatrix}$\n",
    "\n",
    "Which equals:\n",
    "$\\beta$ = ($X$$^T$$X$+$\\lambda$I)$^-1$$X$$^T$$Y$\n",
    "\n",
    "This last expression is the ridge estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.3 Can we now solve the $p > n$ situation? Explain why. **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, we can. The main problem when p>n is that there are an infinite number of solutions that could go through a small number of data points compared to the number of predictors. However, if we penalize high coefficients, then we further constrain the model and limit the number of possible solutions. We can solve the problem using the ridge estimator calculated above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.4 Take a look at the LASSO estimator expression that we derived when $X^TX=I$. What needs to happen for LASSO to nullify  $\\beta_i$? **\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For LASSO to nullify $\\beta_i$, the corresponding $|x_i^Ty|$ has to be smaller than $\\lambda^T$ ie. $\\lambda/2$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.5  Can LASSO be used when $p>n$? What important consideration, related to the number of predictors that LASSO chooses, do we have to keep in mind in that case? **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LASSO can be used in the p>n case. However, if used for p>n, LASSO only uses n predictors and nullifies the remaining (p - n) predictors. When predictors are correlated, LASSO picks one and nullifies the rest, which is why different runs could generate different sets of predictors.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.6 Ridge and LASSO still have room for improvement. List two limitations of Ridge, and two limitations of LASSO. **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limitations of ridge:\n",
    "- while Ridge reduces variance in the model, it also adds bias\n",
    "- doesn't actually discard irrelevant predictors from the model: only minimizes (ie. no feature selection)\n",
    "\n",
    "Limitations of LASSO:\n",
    "- when p>n, LASSO uses at most n predictors and discards the rest\n",
    "- when predictors are correlated, LASSO picks one predictor in an arbitrary way, which means different runs may result in different models\n",
    "\n",
    "Plus, both are sensible to outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.7 Review the class slides and answer the following questions: When is Ridge preferred? When is LASSO preferred? When is Elastic Net preferred? **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge is preferred when we have a high number of informative predictors since, contrary to LASSO, it does not do feature selection. It is also preferred when we would rather shrink correlated predictors over picking one arbitrarily (as is done by LASSO).\n",
    "LASSO is preferred when we have a powerful predictor which would otherwise be shrunk disproportionately by Ridge and when we have few informative predictors (due to its feature selection capabilities). \n",
    "Elastic Net is a good compromise between Ridge and Lasso. It increases stability, reduces model complexity and performs feature selection. For example, it tries to do predictor selection and minimization simultaneously. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise'><b> Question 2 [12pts]</b></div>\n",
    "\n",
    "We want to analyze the behavior of our estimators in cases where p > n. We will generate dummy regression problems for this analysis, so that we have full control on the properties of the problem. Sklearn provides an easy to use function to generate regression problems: `sklearn.datasets.make_regression`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1** Use the provided notebook cell to to build a dataset with 500 samples, 2500 features, 100 informative features and a noise sd of 10.0. The function will return the true coefficients in `true_coef`. Intercepts are not generated, so do not fit them in your regressions. Fit LinearRegression, LassoCV, RidgeCV and ElasticNetCV estimators on the traininig set with 5-fold crossvalidation.\n",
    "\n",
    "Test 100 lambda values from 0.01 to 1000, in logscale. For Elastic Net, also test the following L1 ratios: [.1, .5, .7, .9, .95, .99] (it is good practice to try more ratio values near the L1 term, as the ridge penalty tends to have higher absolute magnitude).\n",
    "\n",
    "**Do not change `random_state=209`, to facilitate grading.**\n",
    "\n",
    "**2.2** As we used `n_informative = 100`, the true betas will contain 100 non-zero values. Let's see if our estimators picked up on that trend. Print the number of betas greater than $10^{-6}$ (non-zero values) for each estimator, and comment on the results.\n",
    "\n",
    "**2.3**  Let's see how our estimators perform on the test set. Calculate $R^2$ for each estimator on the test set. Comment on the results.\n",
    "\n",
    "**2.4** Now, let's observe what happens when we  increase the number of informative features. Generate another regression problem with the same parameters as before, but this time with an n_informative of 600. Finally, fit OLS, Ridge, LASSO and EN, and print the number of non-zero coefficients and R2 Scores.\n",
    "\n",
    "\n",
    "**2.5**  Compare the results with the previous case and comment. What can we say about LASSO and Elastic Net in particular?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "n= 500\n",
    "p= 2500\n",
    "informative= 100\n",
    "rs = 209\n",
    "sd = 5\n",
    "\n",
    "# Generate regresion\n",
    "X,y,true_coef = make_regression(n_samples = n, n_features = p, n_informative = informative,\n",
    "                                coef = True, noise = sd)\n",
    "\n",
    "# Get train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=rs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1 Use the provided notebook cell to to build a dataset with 500 samples, 2500 features, 100 informative features and a noise sd of 10.0. The function will return the true coefficients in `true_coef`. Intercepts are not generated, so do not fit them in your regressions. Fit LinearRegression, LassoCV, RidgeCV and ElasticNetCV estimators on the traininig set with 5-fold crossvalidation. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Coefficients: [0. 0. 0. ... 0. 0. 0.]\n",
      "OLS Coefficients: [ 2.49532172 -3.88109851 -1.05746322 ...  3.43275122 -1.93670282\n",
      "  5.96516204]\n",
      "LASSO Coefficients: [ 0.         -3.2768493  -0.         ...  0.         -0.\n",
      "  3.72692421]\n",
      "Ridge Coefficients: [ 2.4953063  -3.88107855 -1.05744828 ...  3.43273865 -1.93670633\n",
      "  5.96513758]\n",
      "Elastic Net Coefficients: [ 0. -0.  0. ...  0. -0.  0.]\n"
     ]
    }
   ],
   "source": [
    "lambdas = np.logspace(-2, 3, 100, base = 10)\n",
    "L1_ratios = [0.1, 0.5, 0.7, 0.9, 0.95, 0.99] \n",
    "\n",
    "print(\"True Coefficients: {}\".format(true_coef))\n",
    "\n",
    "#fit OLS\n",
    "lm = LinearRegression(fit_intercept = False)\n",
    "lm_model = lm.fit(X_train, y_train)\n",
    "lm_coefficients = lm.coef_\n",
    "print(\"OLS Coefficients: {}\".format(lm_coefficients))\n",
    "\n",
    "#fit LassoCV\n",
    "la = LassoCV(alphas= lambdas, cv = 5, fit_intercept = False)\n",
    "la_model = la.fit(X_train, y_train)\n",
    "la_coefficients = la.coef_\n",
    "print(\"LASSO Coefficients: {}\".format(la_coefficients))\n",
    "\n",
    "#fit RidgeCV\n",
    "ri = RidgeCV(alphas = lambdas, cv = 5, fit_intercept = False)\n",
    "ri_model = ri.fit(X_train, y_train)\n",
    "ri_coefficients = ri.coef_\n",
    "print(\"Ridge Coefficients: {}\".format(ri_coefficients))\n",
    "\n",
    "#fit ElasticNetCV\n",
    "en = ElasticNetCV(alphas = lambdas, cv = 5, fit_intercept = False, l1_ratio = L1_ratios, max_iter = 10000)\n",
    "en_model = en.fit(X_train, y_train)\n",
    "en_coefficients = en.coef_\n",
    "print(\"Elastic Net Coefficients: {}\".format(en_coefficients))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2 As we used `n_informative = 100`, the true betas will contain 100 non-zero values. Let's see if our estimators picked up on that trend. Print the number of betas with absolute value greater than $10^{-6}$ (which will corrspond to non-zero values) for each estimator, and comment on the results. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Betas with absolute value greater than 1e-6 for OLS: 2500\n",
      "Number of Betas with absolute value greater than 1e-6 for LASSO: 315\n",
      "Number of Betas with absolute value greater than 1e-6 for Ridge: 2500\n",
      "Number of Betas with absolute value greater than 1e-6 for Elastic Net: 324\n",
      "Number of Betas with absolute value greater than 1e-6 for Actual Data: 100\n"
     ]
    }
   ],
   "source": [
    "th = 1e-6 #threshold\n",
    "\n",
    "#counts keeping track of how many beta estimates exceed the threshold for each model\n",
    "lm_coef_count = 0\n",
    "la_coef_count = 0\n",
    "ri_coef_count = 0\n",
    "en_coef_count = 0\n",
    "true_coef_count = 0\n",
    "\n",
    "#loops through arrays of coefficients and counts number of coefficients who's absolute values exceed the threshold\n",
    "for coef in lm_coefficients:\n",
    "    if abs(coef) > th:\n",
    "        lm_coef_count += 1\n",
    "        \n",
    "for coef in la_coefficients:\n",
    "    if abs(coef) > th:\n",
    "        la_coef_count += 1\n",
    "\n",
    "for coef in ri_coefficients:\n",
    "    if abs(coef) > th:\n",
    "        ri_coef_count += 1\n",
    "\n",
    "for coef in en_coefficients:\n",
    "    if abs(coef) > th:\n",
    "        en_coef_count += 1\n",
    "        \n",
    "for coef in true_coef:\n",
    "    if abs(coef) > th:\n",
    "        true_coef_count += 1\n",
    "        \n",
    "#print statements\n",
    "print(\"Number of Betas with absolute value greater than 1e-6 for OLS: {}\".format(lm_coef_count))\n",
    "print(\"Number of Betas with absolute value greater than 1e-6 for LASSO: {}\".format(la_coef_count))\n",
    "print(\"Number of Betas with absolute value greater than 1e-6 for Ridge: {}\".format(ri_coef_count))\n",
    "print(\"Number of Betas with absolute value greater than 1e-6 for Elastic Net: {}\".format(en_coef_count))\n",
    "print(\"Number of Betas with absolute value greater than 1e-6 for Actual Data: {}\".format(true_coef_count))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the results above, we see that LASSO and Elastic Net are the only two models that do feature selection and actually nullify irrelevant predictors. LASSO is the one that gets closest to the 100 from the actual data set (ie. 282 predictors) and Elastic Net is a compromise between LASSO and Ridge and thus less selective than the LASSO model (ie. 648 predictors). Ridge and OLS fail to nullify predictors altogether (ie. keep all 2500 predictors).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.3**  Let's see how our estimators perform on the test set. Calculate $R^2$ for each estimator on the test set. Comment on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 score for OLS is: 0.07663861080466006\n",
      "R^2 score for LASSO is: 0.6934720987576901\n",
      "R^2 score for Ridge is: 0.07663863662700066\n",
      "R^2 score for Elastic Net is: 0.46975188657566647\n"
     ]
    }
   ],
   "source": [
    "lm_predictions = lm.predict(X_test)\n",
    "lm_r2 = r2_score(y_test, lm_predictions)\n",
    "print(\"R^2 score for OLS is: {}\".format(lm_r2))\n",
    "\n",
    "la_predictions = la.predict(X_test)\n",
    "la_r2 = r2_score(y_test, la_predictions)\n",
    "print(\"R^2 score for LASSO is: {}\".format(la_r2))\n",
    "\n",
    "ri_predictions = ri.predict(X_test)\n",
    "ri_r2 = r2_score(y_test, ri_predictions)\n",
    "print(\"R^2 score for Ridge is: {}\".format(ri_r2))\n",
    "\n",
    "en_predictions = en.predict(X_test)\n",
    "en_r2 = r2_score(y_test, en_predictions)\n",
    "print(\"R^2 score for Elastic Net is: {}\".format(en_r2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model with the best $R^2$ score is LASSO and the worst ones are OLS and Ridge. This is expected since LASSO nullifies features that are not as relevant or not relevant while Ridge only shrinks them. As such, we'd expect LASSO to fit the dataset better. Elastic Net is a compromise between Ridge and LASSO and thus has an $R^2$ score in between the two."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.4 Now, let's observe what happens when we  increase the number of informative features. Generate another regression problem with the same parameters as before, but this time with an n_informative of 600. Finally, fit OLS, Ridge, LASSO and EN, and print the number of non-zero coefficients and R2 Scores. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Coefficients: [ 0.         52.19919716  0.         ...  0.         75.92265075\n",
      "  0.        ]\n",
      "OLS Coefficients: [ 7.55308533  9.87807871 19.94142677 ... 15.6907177  20.38565803\n",
      "  4.35790212]\n",
      "LASSO Coefficients: [0. 0. 0. ... 0. 0. 0.]\n",
      "Ridge Coefficients: [ 4.93303011  6.21455076 14.81498734 ... 12.02175632 13.77286597\n",
      "  3.07392367]\n",
      "Elastic Net Coefficients: [ 4.52002447  5.95294536 15.31407695 ... 12.0871949  14.32904183\n",
      "  2.61505037]\n",
      "Number of Betas with absolute value greater than 1e-6 for Linear Regression: 2500\n",
      "Number of Betas with absolute value greater than 1e-6 for LASSO: 29\n",
      "Number of Betas with absolute value greater than 1e-6 for Ridge: 2500\n",
      "Number of Betas with absolute value greater than 1e-6 for Elastic Net: 2302\n",
      "Number of Betas with absolute value greater than 1e-6 for Actual Data: 600\n",
      "R^2 score for OLS is: 0.20947181541155235\n",
      "R^2 score for LASSO is: 0.03963918103235031\n",
      "R^2 score for Ridge is: 0.1741048597324616\n",
      "R^2 score for Elastic Net is: 0.17479361406155858\n"
     ]
    }
   ],
   "source": [
    "informative_new= 600\n",
    "\n",
    "# Generate regresion\n",
    "X,y,true_coef = make_regression(n_samples = n, n_features = p, n_informative = informative_new,\n",
    "                                coef = True, noise = sd)\n",
    "\n",
    "# Get train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=rs)\n",
    "\n",
    "print(\"True Coefficients: {}\".format(true_coef))\n",
    "\n",
    "\n",
    "##Fit models\n",
    "#fit OLS\n",
    "lm = LinearRegression(fit_intercept = False)\n",
    "lm_model = lm.fit(X_train, y_train)\n",
    "lm_coefficients = lm.coef_\n",
    "print(\"OLS Coefficients: {}\".format(lm_coefficients))\n",
    "\n",
    "#fit LassoCV\n",
    "la = LassoCV(alphas= lambdas, cv = 5, fit_intercept = False)\n",
    "la_model = la.fit(X_train, y_train)\n",
    "la_coefficients = la.coef_\n",
    "print(\"LASSO Coefficients: {}\".format(la_coefficients))\n",
    "\n",
    "#fit RidgeCV\n",
    "ri = RidgeCV(alphas = lambdas, cv = 5, fit_intercept = False)\n",
    "ri_model = ri.fit(X_train, y_train)\n",
    "ri_coefficients = ri.coef_\n",
    "print(\"Ridge Coefficients: {}\".format(ri_coefficients))\n",
    "\n",
    "#fit ElasticNetCV\n",
    "en = ElasticNetCV(alphas = lambdas, cv = 5, fit_intercept = False, l1_ratio = L1_ratios, max_iter = 10000)\n",
    "en_model = en.fit(X_train, y_train)\n",
    "en_coefficients = en.coef_\n",
    "print(\"Elastic Net Coefficients: {}\".format(en_coefficients))\n",
    "\n",
    "\n",
    "##Number of Betas above 1e-6 threshold\n",
    "th = 1e-6 #threshold\n",
    "\n",
    "#counts keeping track of how many beta estimates exceed the threshold for each model\n",
    "lm_coef_count = 0\n",
    "la_coef_count = 0\n",
    "ri_coef_count = 0\n",
    "en_coef_count = 0\n",
    "true_coef_count = 0\n",
    "\n",
    "#loops through arrays of coefficients and counts number of coefficients who's absolute values exceed the threshold\n",
    "for coef in lm_coefficients:\n",
    "    if abs(coef) > th:\n",
    "        lm_coef_count += 1\n",
    "        \n",
    "for coef in la_coefficients:\n",
    "    if abs(coef) > th:\n",
    "        la_coef_count += 1\n",
    "\n",
    "for coef in ri_coefficients:\n",
    "    if abs(coef) > th:\n",
    "        ri_coef_count += 1\n",
    "\n",
    "for coef in en_coefficients:\n",
    "    if abs(coef) > th:\n",
    "        en_coef_count += 1\n",
    "        \n",
    "for coef in true_coef:\n",
    "    if abs(coef) > th:\n",
    "        true_coef_count += 1\n",
    "        \n",
    "#print statements\n",
    "print(\"Number of Betas with absolute value greater than 1e-6 for Linear Regression: {}\".format(lm_coef_count))\n",
    "print(\"Number of Betas with absolute value greater than 1e-6 for LASSO: {}\".format(la_coef_count))\n",
    "print(\"Number of Betas with absolute value greater than 1e-6 for Ridge: {}\".format(ri_coef_count))\n",
    "print(\"Number of Betas with absolute value greater than 1e-6 for Elastic Net: {}\".format(en_coef_count))\n",
    "print(\"Number of Betas with absolute value greater than 1e-6 for Actual Data: {}\".format(true_coef_count))\n",
    "\n",
    "\n",
    "\n",
    "##R^2 scores\n",
    "lm_predictions = lm.predict(X_test)\n",
    "lm_r2 = r2_score(y_test, lm_predictions)\n",
    "print(\"R^2 score for OLS is: {}\".format(lm_r2))\n",
    "\n",
    "la_predictions = la.predict(X_test)\n",
    "la_r2 = r2_score(y_test, la_predictions)\n",
    "print(\"R^2 score for LASSO is: {}\".format(la_r2))\n",
    "\n",
    "ri_predictions = ri.predict(X_test)\n",
    "ri_r2 = r2_score(y_test, ri_predictions)\n",
    "print(\"R^2 score for Ridge is: {}\".format(ri_r2))\n",
    "\n",
    "en_predictions = en.predict(X_test)\n",
    "en_r2 = r2_score(y_test, en_predictions)\n",
    "print(\"R^2 score for Elastic Net is: {}\".format(en_r2))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.5  Compare the results with the previous case and comment. What can we say about LASSO and Elastic Net in particular? **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing this case (informative predictors = 600) with the previous one (informative predictors = 100), we can see that LASSO now has a negative $R^2$ value and Elastic Net now has the highest $R^2$ value. In other words, LASSO is the worst model when compared to OLS, Ridge and EN in the case where there are more informative predictors. This is expected because LASSO performs feature selection and thus nullifies lots of informative predictors (only 40 out of 600 remain) while Ridge and OLS only shrink them (keep all 2500). Elastic Net gives us the best of both worlds between Ridge and LASSO and thus has the best $R^2$ score with a large number of informative predictors (only keeps 2479 out of 2500). While it isn't a perfect model, it is the best out of the 4 we tested.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise'><b> Question 3 [1pt] (for fun) </b></div>\n",
    "\n",
    "We would like to visualize how Ridge, LASSO and Elastic Net behave. We will build a toy regression example to observe the behavior of the coefficients and loss function as lambda increases.\n",
    "\n",
    "**3.1** Use `sklearn.datasets.make_regression` to build a well-conditioned regression problem with 1000 samples, 5 features, noise standard deviation of 10 and random state 209.\n",
    "\n",
    "**3.2** Find the Ridge, LASSO and EN estimator for this problem, varying the regularization parameter in the interval $[0.1,100]$ for LASSO and EN, and $[0.1,10000]$ for Ridge. Plot the evolution of the 5 coefficients for each estimator in a 2D plot, where the X axis is the regularization parameter and the Y axis is the coefficient value. For Elastic Net, make 4 plots, each one with one of the following L1 ratios: $[0.1, 0.5, 0.8, 0.95]$ You should have 6 plots: one for Lasso, one for Ridge, and 4 for EN. Each plot should have 5 curves, one per coefficient. \n",
    "\n",
    "**3.3** Comment on this evolution. Does this make sense with what we've seen so far?\n",
    "\n",
    "**3.4** We're now interested in visualizing the behavior of the Loss functions. First, generate a regression problem with 1000 samples and 2 features. Then, use the provided \"loss_3d_interactive\" function to observe how the loss surface changes as the regularization parameter changes. Test the function with Ridge_loss, LASSO_loss and EN_loss. Comment on what you observe.**\n",
    "\n",
    "**Note: for this to work, you have to install plotly. Go to https://plot.ly/python/getting-started/ and follow the steps. You don't need to make an account as we'll use the offline mode.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solutions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.1 Use `sklearn.datasets.make_regression` to build a well-conditioned regression problem with 1000 samples, 5 features, noise standard deviation of 10 and random state 209. **\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.2 Find the Ridge, LASSO and EN estimator for this problem, varying the regularization parameter in the interval $[0.1,100]$ for LASSO and EN, and $[0.1,10000]$ for Ridge. Plot the evolution of the 5 coefficients for each estimator in a 2D plot, where the X axis is the regularization parameter and the Y axis is the coefficient value. For Elastic Net, make 4 plots, each one with one of the following L1 ratios: $[0.1, 0.5, 0.8, 0.95]$ You should have 6 plots: one for Lasso, one for Ridge, and 4 for EN. Each plot should have 5 curves, one per coefficient. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.3 Comment on this evolution. Does this make sense with what we've seen so far?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here* \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.4 We're now interested in visualizing the behavior of the Loss functions. First, generate a regression problem with 1000 samples and 2 features. Then, use the provided \"loss_3d_interactive\" function to observe how the loss surface changes as the regularization parameter changes. Test the function with Ridge_loss, LASSO_loss and EN_loss. Comment on what you observe.**\n",
    "\n",
    "**Note: for this to work, you have to install plotly. Go to https://plot.ly/python/getting-started/ and follow the steps. You don't need to make an account as we'll use the offline mode.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X,y,true_coef = make_regression(n_samples = 1000, n_features = 2, noise = 10, random_state=209, coef=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ipywidgets import interactive, HBox, VBox\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "import plotly.graph_objs as go\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "def OLS_loss(X, y, beta, lbda=0):\n",
    "    y_hat = np.dot(X,beta)\n",
    "    return np.sum((y_hat-y)**2,axis=0)\n",
    "\n",
    "def Ridge_loss(X, y, beta, lbda):\n",
    "    y_hat = np.dot(X,beta)\n",
    "    return np.sum((y_hat-y)**2,axis=0) + lbda*np.sum(beta**2, axis=0)\n",
    "\n",
    "def LASSO_loss(X, y, beta, lbda):\n",
    "    y_hat = np.dot(X,beta)\n",
    "    return (1 / (2 * len(X)))*np.sum((y_hat-y)**2,axis=0) + lbda*np.sum(np.abs(beta), axis=0)\n",
    "\n",
    "def EN_loss(X, y, beta, lbda):\n",
    "    ratio=0.1\n",
    "    y_hat = np.dot(X,beta)\n",
    "    return (1 / (2 * len(X)))*np.sum((y_hat-y)**2,axis=0) + lbda*(ratio*np.sum(beta**2, axis=0) + (1-ratio)*np.sum(np.abs(beta), axis=0))\n",
    "\n",
    "def loss_3d_interactive(X, y, loss='Ridge'):\n",
    "    '''Uses plotly to draw an interactive 3D representation of the loss function, \n",
    "    with a slider to control the regularization factor.\n",
    "    \n",
    "    Inputs:\n",
    "    X: predictor matrix for the regression problem. Has to be of dim n x 2\n",
    "    y: response vector \n",
    "    \n",
    "    loss: string with the loss to plot. Options are 'Ridge', 'LASSO', 'EN'.\n",
    "    '''\n",
    "    \n",
    "    if loss == 'Ridge':\n",
    "        loss_function = Ridge_loss\n",
    "        lbda_slider_min = 0\n",
    "        lbda_slider_max = 10000\n",
    "        lbda_step = 10\n",
    "        clf = Ridge()\n",
    "    elif loss == 'LASSO':\n",
    "        loss_function = LASSO_loss\n",
    "        lbda_slider_min = 1\n",
    "        lbda_slider_max = 150\n",
    "        lbda_step = 1\n",
    "        clf = Lasso()\n",
    "    elif loss == 'EN':\n",
    "        loss_function = EN_loss\n",
    "        lbda_slider_min = 1\n",
    "        lbda_slider_max = 150\n",
    "        lbda_step = 1\n",
    "        clf = ElasticNet()\n",
    "    else:\n",
    "        raise ValueError(\"Loss string not recognized. Available options are: 'Ridge', 'LASSO', 'EN'.\")\n",
    "        \n",
    "    \n",
    "    # linspace for loss surface\n",
    "    L=20\n",
    "    lsp_b = np.linspace(-80,80,L)\n",
    "    lsp_b_x, lsp_b_y = np.meshgrid(lsp_b,lsp_b)\n",
    "    lsp_b_mat = np.column_stack((lsp_b_x.flatten(),lsp_b_y.flatten()))\n",
    "    \n",
    "    # Get all optimal betas for current lambda range\n",
    "    precomp_coefs=[]\n",
    "    for l in range(lbda_slider_min,lbda_slider_max+1,lbda_step):\n",
    "        clf.set_params(alpha=l)\n",
    "        clf.fit(X, y)\n",
    "        precomp_coefs.append(clf.coef_)\n",
    "                \n",
    "    f = go.FigureWidget(\n",
    "        data=[\n",
    "            go.Surface(\n",
    "                    x=lsp_b_x,\n",
    "                    y=lsp_b_y,\n",
    "                    z=loss_function(X,y.reshape(-1,1), lsp_b_mat.T, 0).reshape((L,L)),\n",
    "                    colorscale='Viridis',\n",
    "                    opacity=0.7,\n",
    "                    contours=dict(z=dict(show=True,\n",
    "                                         width=3,\n",
    "                                         highlight=True,\n",
    "                                         highlightcolor='orange',\n",
    "                                         project=dict(z=True),\n",
    "                                         usecolormap=True))\n",
    "            ),\n",
    "            \n",
    "            go.Scatter3d(\n",
    "                x=[p[0] for p in precomp_coefs],\n",
    "                y=[p[1] for p in precomp_coefs],\n",
    "                z=np.zeros(len(precomp_coefs)),\n",
    "                marker=dict(\n",
    "                    size=1,\n",
    "                    color='darkorange',\n",
    "                    line=dict(\n",
    "                        color='darkorange',\n",
    "                        width=1\n",
    "                        ),\n",
    "                    opacity=1\n",
    "                    )\n",
    "                ),\n",
    "            go.Scatter3d(\n",
    "                x=[0],\n",
    "                y=[0],\n",
    "                z=[0],\n",
    "                \n",
    "                marker=dict(\n",
    "                    size=10,\n",
    "                    color='orange',\n",
    "                    opacity=1\n",
    "                    ),\n",
    "            )\n",
    "        ],\n",
    "\n",
    "        layout=go.Layout(scene=go.layout.Scene(\n",
    "                    xaxis = dict(\n",
    "                        title='Beta 1'),\n",
    "                    yaxis = dict(\n",
    "                        title='Beta 2'),\n",
    "                    zaxis = dict(\n",
    "                        title='Loss'),\n",
    "            camera=go.layout.scene.Camera(\n",
    "                up=dict(x=0, y=0, z=1),\n",
    "                center=dict(x=0, y=0, z=0),\n",
    "                eye=dict(x=1.25, y=1.25, z=1.25))\n",
    "        ),\n",
    "            width=1000,\n",
    "            height=700,)\n",
    "    )\n",
    "\n",
    "    def update_z(lbda):\n",
    "        f.data[0].z = loss_function(X, y.reshape(-1,1), lsp_b_mat.T, lbda).reshape((L,L))\n",
    "        beta_opt = precomp_coefs[(lbda-lbda_slider_min)//(lbda_step)]\n",
    "        f.data[-1].x = [beta_opt[0]]\n",
    "        f.data[-1].y = [beta_opt[1]]\n",
    "        f.data[-1].z = [0]\n",
    "\n",
    "    lambda_slider = interactive(update_z, lbda=(lbda_slider_min, lbda_slider_max, lbda_step))\n",
    "    vb = VBox((f, lambda_slider))\n",
    "    vb.layout.align_items = 'center'\n",
    "    display(vb)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
